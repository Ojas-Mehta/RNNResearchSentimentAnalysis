{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAToy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ojas-Mehta/RNNResearchSentimentAnalysis/blob/master/SAToy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSqiB0jwQcgr",
        "colab_type": "code",
        "outputId": "d49f84a2-4e99-461d-b9da-ed9c96048bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po1TptY8WhBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import json\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUTzBcALhpn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ########Generate synthetic data\n",
        "#torch.manual_seed(2109)\n",
        "# #train_synth.csv, test_synth.csv\n",
        "# import numpy as np\n",
        "# import csv\n",
        "# dataset = []\n",
        "# dataset_size = 2000\n",
        "# for i in range(dataset_size):\n",
        "#     a = np.floor(100*np.random.rand(10)).astype(int)\n",
        "#     if(((80 <= a) & (a < 89)).sum()>=((90 <= a) & (a < 100)).sum()):\n",
        "#       label = 1\n",
        "#     else:\n",
        "#       label = 0\n",
        "#     dataset.append([a.tolist(),label])\n",
        "# with open('/content/drive/My Drive/sentiment/data/val_small.csv', 'w') as csvFile:\n",
        "#     writer = csv.writer(csvFile)\n",
        "#     writer.writerows(dataset)\n",
        "# csvFile.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBJ39LSkBZ6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read data\n",
        "#Train = 15000, Val = 2000, Test = 3000\n",
        "train_data = pd.read_csv('/content/drive/My Drive/sentiment/data/train_small.csv', header = None)\n",
        "val_data = pd.read_csv('/content/drive/My Drive/sentiment/data/val_small.csv', header = None)\n",
        "test_data = pd.read_csv('/content/drive/My Drive/sentiment/data/test_small.csv', header = None)\n",
        "train_X_list = train_data[0].tolist()\n",
        "train_y = train_data[1].tolist()\n",
        "val_X_list = val_data[0].tolist()\n",
        "val_y = val_data[1].tolist()\n",
        "test_X_list = test_data[0].tolist()\n",
        "test_y = test_data[1].tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRFfV-AUybLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocesssing:String to Numeric\n",
        "torch.manual_seed(2109)\n",
        "def stringToList(string):\n",
        "    string = string[1:len(string)-1]\n",
        "    try:\n",
        "        if len(string) != 0: \n",
        "            tempList = string.split(\", \")\n",
        "            newList = list(map(lambda x: int(x), tempList))\n",
        "        else:\n",
        "            newList = []\n",
        "    except:\n",
        "        newList = [-9999]\n",
        "    return(newList)\n",
        "train_X_list = [stringToList(x) for x in train_X_list]\n",
        "train_X = torch.tensor(train_X_list, dtype=torch.long)\n",
        "val_X_list = [stringToList(x) for x in val_X_list]\n",
        "val_X = torch.tensor(val_X_list, dtype=torch.long)\n",
        "test_X_list = [stringToList(x) for x in test_X_list]\n",
        "test_X = torch.tensor(test_X_list, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWGMDRiZCLI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6944fc20-5f4d-48b6-f15b-5ce135afaa0c"
      },
      "source": [
        "len(test_X_list)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjl6NLFQfK38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define hyperparameters\n",
        "input_dim = 10\n",
        "output_dim = 2\n",
        "n_epochs = 150\n",
        "batch_size = 100\n",
        "lr = 0.001\n",
        "hidden_dimension = 8\n",
        "patience_param = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXizYy5pjKxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get embeddings for data\n",
        "torch.manual_seed(2109)\n",
        "embeddings = nn.Embedding(100, input_dim)\n",
        "train_input = embeddings(train_X)\n",
        "train_output = torch.tensor(train_y, dtype=torch.long)\n",
        "\n",
        "val_input = embeddings(val_X)\n",
        "val_output = torch.tensor(val_y, dtype=torch.long)\n",
        "test_input = embeddings(test_X)\n",
        "result = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-LmbDMhdZ0z",
        "colab_type": "code",
        "outputId": "da784d6a-783e-4317-bca6-450e26b82999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# %%javascript\n",
        "# IPython.OutputArea.auto_scroll_threshold = 9999;\n",
        "#########Inference from the saved model\n",
        "batch_size = 15000\n",
        "def evaluate(tempmodel, dataX, dataY, criterion, batch_size):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    correct_pred = 0\n",
        "    tempmodel.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, dataX.shape[0], batch_size):\n",
        "            batchX, batchy = dataX[i:i+batch_size], dataY[i:i+batch_size]\n",
        "            batchX = batchX.to(device)\n",
        "            batchy = batchy.to(device)\n",
        "            output, hidden = tempmodel(batchX, batch_size = batch_size)          \n",
        "            loss = criterion(hidden, batchy)\n",
        "\n",
        "            epoch_loss += loss.item()*batch_size\n",
        "            _, predicted_labels = torch.max(hidden, 1)\n",
        "            correct_pred += (predicted_labels==batchy).sum().item()\n",
        "        accuracy = correct_pred/ len(dataX)*100\n",
        "        epoch_loss /= len(dataX)   \n",
        "    return epoch_loss, accuracy\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")\n",
        "    \n",
        "    \n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers, batch_size):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x, batch_size):\n",
        "        \n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        hidden = hidden.transpose(0,1).contiguous().view(batch_size, -1)\n",
        "        hidden = self.fc(hidden)\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        hidden = hidden.to(device)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "\n",
        "# Create and Load the model\n",
        "batch_size = 15000\n",
        "model = Model(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dimension, n_layers=1, batch_size = batch_size)\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/sentiment/Model/myModelsmall8.pt'))\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "#print Train Accuracy\n",
        "batch_X, batch_y = train_input[0:batch_size], train_output[0:batch_size]\n",
        "batch_X = batch_X.to(device)\n",
        "batch_y = batch_y.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "printLoss, printAccuracy = evaluate(model, train_input, train_output, criterion, batch_size)\n",
        "print(printLoss, printAccuracy)\n",
        "\n",
        "#print Validation Accuracy\n",
        "batch_size = 2000\n",
        "batch_X, batch_y = val_input[0:batch_size], val_output[0:batch_size]\n",
        "batch_X = batch_X.to(device)\n",
        "batch_y = batch_y.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "printLoss, printAccuracy = evaluate(model, val_input, val_output, criterion, batch_size)\n",
        "print(printLoss, printAccuracy)\n",
        "\n",
        "# #print Single Example hidden states\n",
        "# batch_size = 1\n",
        "# # batch_X, batch_y = val_input[3:4], val_output[3:4]\n",
        "# batch_X = custom_example\n",
        "# batch_X = batch_X.to(device)\n",
        "# output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n",
            "0.08412731438875198 97.7\n",
            "0.098625548183918 97.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-acbe184d1377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# batch_X, batch_y = val_input[3:4], val_output[3:4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'custom_example' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckFcORv0trOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Get parameters of the loaded model\n",
        "ordir = model.state_dict()\n",
        "B = ordir['rnn.weight_ih_l0'].cpu()\n",
        "B_bias = ordir['rnn.bias_ih_l0'].cpu()\n",
        "A = ordir['rnn.weight_hh_l0'].cpu()\n",
        "A_bias = ordir['rnn.bias_hh_l0'].cpu()\n",
        "C = ordir['fc.weight'].cpu()\n",
        "C_bias = ordir['fc.bias'].cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6krWSt6uAXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ##############################Product of parameter and input word vector\n",
        "# for i in range(100):\n",
        "#     inputword = embeddings(torch.tensor(i, dtype=torch.long))\n",
        "#     print(torch.norm(torch.mm(B,inputword.view(10,1))))#default frob. norm\n",
        "# #     print(torch.sum(torch.mm(InputParam,inputword.view(10,1))+InputBias.view(8,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kLNRxcWakYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8a067c7f-d138-4cd0-e405-e7fa71dd4c0c"
      },
      "source": [
        "custom_example1 = embeddings(torch.tensor([[91, 92, 93, 91, 92, 93, 91, 92, 93, 92]], dtype = torch.long))###all negative\n",
        "custom_example2 = embeddings(torch.tensor([[81, 82, 83, 81, 82, 83, 81, 82, 83, 82]], dtype = torch.long))###all positive\n",
        "custom_example3 = embeddings(torch.tensor([[21, 2, 3, 11, 22, 33, 41, 52, 63, 72]], dtype = torch.long))###all neutral\n",
        "custom_example4 = embeddings(torch.tensor([[82, 91, 83, 98, 81, 92, 83, 91, 82, 93]], dtype = torch.long))###alternate\n",
        "\n",
        "#print Single Example hidden states\n",
        "batch_size = 1\n",
        "# batch_X, batch_y = val_input[3:4], val_output[3:4]\n",
        "print('NORM of hidden state for [91, 92, 93, 91, 92, 93, 91, 92, 93, 92]------')\n",
        "batch_X = custom_example1\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.norm(output[0][i]))\n",
        "print('\\nNORM--[81, 82, 83, 81, 82, 83, 81, 82, 83, 82]--------')\n",
        "batch_X = custom_example2\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.norm(output[0][i]))\n",
        "  batch_X = custom_example1\n",
        "print('-\\nNORM---[21, 2, 3, 11, 22, 33, 41, 52, 63, 72]------')\n",
        "batch_X = custom_example3\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.norm(output[0][i]))\n",
        "  batch_X = custom_example1\n",
        "print('\\nNORM----[82, 91, 83, 98, 81, 92, 83, 91, 82, 93]----')\n",
        "batch_X = custom_example4\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.norm(output[0][i]))\n",
        "\n",
        "  \n",
        "\n",
        "print('\\nSUM of hidden state----[91, 92, 93, 91, 92, 93, 91, 92, 93, 92]-----')\n",
        "batch_X = custom_example1\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.sum(output[0][i]))\n",
        "print('-\\nSUM--[81, 82, 83, 81, 82, 83, 81, 82, 83, 82]---')\n",
        "batch_X = custom_example2\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.sum(output[0][i]))\n",
        "  batch_X = custom_example1\n",
        "print('-\\nSUM--[21, 2, 3, 11, 22, 33, 41, 52, 63, 72]--------')\n",
        "batch_X = custom_example3\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.sum(output[0][i]))\n",
        "  batch_X = custom_example1\n",
        "print('-\\nSUM--[82, 91, 83, 98, 81, 92, 83, 91, 82, 93]------')\n",
        "batch_X = custom_example4\n",
        "batch_X = batch_X.to(device)\n",
        "output, hidden = model(batch_X, batch_size = batch_size)\n",
        "# print(output)\n",
        "for i in range(10):\n",
        "  print(torch.sum(output[0][i]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NORM of hidden state for [91, 92, 93, 91, 92, 93, 91, 92, 93, 92]------\n",
            "tensor(2.1899, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.0665, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.8245, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.3953, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.2119, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.0407, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.4358, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.2721, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.1057, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.4024, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "\n",
            "NORM--[81, 82, 83, 81, 82, 83, 81, 82, 83, 82]--------\n",
            "tensor(2.2795, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.7783, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.9698, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.5141, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.0058, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.1564, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.5692, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.0893, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.2100, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.3903, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "-\n",
            "NORM---[21, 2, 3, 11, 22, 33, 41, 52, 63, 72]------\n",
            "tensor(1.8673, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.8452, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.1339, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.8693, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.2425, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.0114, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.7285, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.9457, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.5058, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.1586, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "\n",
            "NORM----[82, 91, 83, 98, 81, 92, 83, 91, 82, 93]----\n",
            "tensor(1.7025, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.1042, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.9164, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.9868, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.3004, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.0154, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.9292, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(2.0985, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.7192, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "tensor(1.7283, device='cuda:0', grad_fn=<NormBackward0>)\n",
            "\n",
            "SUM of hidden state----[91, 92, 93, 91, 92, 93, 91, 92, 93, 92]-----\n",
            "tensor(2.1661, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-1.0244, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(0.1932, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.4692, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-0.4656, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(0.7086, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.5257, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-0.2939, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(0.8293, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(0.0918, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "-\n",
            "SUM--[81, 82, 83, 81, 82, 83, 81, 82, 83, 82]---\n",
            "tensor(-2.2187, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.8314, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(3.4149, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-2.0698, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.5119, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.7933, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-1.8492, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.4059, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.6515, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.0759, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "-\n",
            "SUM--[21, 2, 3, 11, 22, 33, 41, 52, 63, 72]--------\n",
            "tensor(-2.1949, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.0033, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-1.3242, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-0.5833, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-0.6467, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1.6167, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-0.0447, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-1.3283, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-1.0699, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-2.8426, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "-\n",
            "SUM--[82, 91, 83, 98, 81, 92, 83, 91, 82, 93]------\n",
            "tensor(2.9018, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1.9386, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(3.8909, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-1.9603, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-2.2285, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-1.5513, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(3.9220, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1.9151, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.8889, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-0.6415, device='cuda:0', grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx2aYvvDan19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}